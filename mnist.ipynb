{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "from models.mnist_resnet50 import resnet50\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "RANDOM_SEED = 1\n",
    "LEARNING_RATE = 0.0001\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 20\n",
    "\n",
    "# Architecture\n",
    "NUM_FEATURES = 28*28\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "# Other\n",
    "DEVICE = \"cuda:0\"\n",
    "GRAYSCALE = True\n",
    "\n",
    "device = torch.device(DEVICE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image batch dimensions: torch.Size([128, 1, 28, 28])\n",
      "Image label dimensions: torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "# Note transforms.ToTensor() scales input images\n",
    "# to 0-1 range\n",
    "train_dataset = datasets.MNIST(root='data', \n",
    "                               train=True, \n",
    "                               transform=transforms.ToTensor(),\n",
    "                               download=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='data', \n",
    "                              train=False, \n",
    "                              transform=transforms.ToTensor())\n",
    "\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, \n",
    "                          batch_size=BATCH_SIZE, \n",
    "                          shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset, \n",
    "                         batch_size=BATCH_SIZE, \n",
    "                         shuffle=False)\n",
    "\n",
    "# Checking the dataset\n",
    "for images, labels in train_loader:  \n",
    "    print('Image batch dimensions:', images.shape)\n",
    "    print('Image label dimensions:', labels.shape)\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar el Modelo Resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "model = resnet50(NUM_CLASSES)\n",
    "model.to(DEVICE)\n",
    " \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fun Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(model, data_loader, device):\n",
    "    correct_pred, num_examples = 0, 0\n",
    "    for i, (features, targets) in enumerate(data_loader):\n",
    "        if(targets.size() == torch.Size([128])):\n",
    "            features = features.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            logits, probas = model(features)\n",
    "            _, predicted_labels = torch.max(probas, 1)\n",
    "            num_examples += targets.size(0)\n",
    "            correct_pred += (predicted_labels == targets).sum()\n",
    "    print(\"correctos: \", correct_pred ,'| total: ',num_examples )\n",
    "    return correct_pred.float() / num_examples * 100"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/020 | Batch 0000/0469 | Cost: 2.5031\n",
      "Epoch: 001/020 | Batch 0050/0469 | Cost: 1.6716\n",
      "Epoch: 001/020 | Batch 0100/0469 | Cost: 1.0220\n",
      "Epoch: 001/020 | Batch 0150/0469 | Cost: 0.9702\n",
      "Epoch: 001/020 | Batch 0200/0469 | Cost: 0.6280\n",
      "Epoch: 001/020 | Batch 0250/0469 | Cost: 0.7149\n",
      "Epoch: 001/020 | Batch 0300/0469 | Cost: 0.4375\n",
      "Epoch: 001/020 | Batch 0350/0469 | Cost: 0.4308\n",
      "Epoch: 001/020 | Batch 0400/0469 | Cost: 0.4524\n",
      "Epoch: 001/020 | Batch 0450/0469 | Cost: 0.2717\n",
      "Epoch: 001/020 | Train: 91.280%\n",
      "Time elapsed: 0.46 min\n",
      "Epoch: 002/020 | Batch 0000/0469 | Cost: 0.2137\n",
      "Epoch: 002/020 | Batch 0050/0469 | Cost: 0.1724\n",
      "Epoch: 002/020 | Batch 0100/0469 | Cost: 0.1615\n",
      "Epoch: 002/020 | Batch 0150/0469 | Cost: 0.2610\n",
      "Epoch: 002/020 | Batch 0200/0469 | Cost: 0.2791\n",
      "Epoch: 002/020 | Batch 0250/0469 | Cost: 0.2688\n",
      "Epoch: 002/020 | Batch 0300/0469 | Cost: 0.2150\n",
      "Epoch: 002/020 | Batch 0350/0469 | Cost: 0.1839\n",
      "Epoch: 002/020 | Batch 0400/0469 | Cost: 0.3181\n",
      "Epoch: 002/020 | Batch 0450/0469 | Cost: 0.1417\n",
      "Epoch: 002/020 | Train: 96.690%\n",
      "Time elapsed: 0.88 min\n",
      "Epoch: 003/020 | Batch 0000/0469 | Cost: 0.1791\n",
      "Epoch: 003/020 | Batch 0050/0469 | Cost: 0.1251\n",
      "Epoch: 003/020 | Batch 0100/0469 | Cost: 0.1162\n",
      "Epoch: 003/020 | Batch 0150/0469 | Cost: 0.2080\n",
      "Epoch: 003/020 | Batch 0200/0469 | Cost: 0.1729\n",
      "Epoch: 003/020 | Batch 0250/0469 | Cost: 0.1625\n",
      "Epoch: 003/020 | Batch 0300/0469 | Cost: 0.1119\n",
      "Epoch: 003/020 | Batch 0350/0469 | Cost: 0.1266\n",
      "Epoch: 003/020 | Batch 0400/0469 | Cost: 0.1357\n",
      "Epoch: 003/020 | Batch 0450/0469 | Cost: 0.1148\n",
      "Epoch: 003/020 | Train: 98.302%\n",
      "Time elapsed: 1.31 min\n",
      "Epoch: 004/020 | Batch 0000/0469 | Cost: 0.1250\n",
      "Epoch: 004/020 | Batch 0050/0469 | Cost: 0.0426\n",
      "Epoch: 004/020 | Batch 0100/0469 | Cost: 0.0480\n",
      "Epoch: 004/020 | Batch 0150/0469 | Cost: 0.0634\n",
      "Epoch: 004/020 | Batch 0200/0469 | Cost: 0.0744\n",
      "Epoch: 004/020 | Batch 0250/0469 | Cost: 0.0958\n",
      "Epoch: 004/020 | Batch 0300/0469 | Cost: 0.0709\n",
      "Epoch: 004/020 | Batch 0350/0469 | Cost: 0.0788\n",
      "Epoch: 004/020 | Batch 0400/0469 | Cost: 0.0225\n",
      "Epoch: 004/020 | Batch 0450/0469 | Cost: 0.0314\n",
      "Epoch: 004/020 | Train: 98.978%\n",
      "Time elapsed: 1.72 min\n",
      "Epoch: 005/020 | Batch 0000/0469 | Cost: 0.0455\n",
      "Epoch: 005/020 | Batch 0050/0469 | Cost: 0.0154\n",
      "Epoch: 005/020 | Batch 0100/0469 | Cost: 0.0673\n",
      "Epoch: 005/020 | Batch 0150/0469 | Cost: 0.0547\n",
      "Epoch: 005/020 | Batch 0200/0469 | Cost: 0.0304\n",
      "Epoch: 005/020 | Batch 0250/0469 | Cost: 0.0929\n",
      "Epoch: 005/020 | Batch 0300/0469 | Cost: 0.0234\n",
      "Epoch: 005/020 | Batch 0350/0469 | Cost: 0.1431\n",
      "Epoch: 005/020 | Batch 0400/0469 | Cost: 0.0490\n",
      "Epoch: 005/020 | Batch 0450/0469 | Cost: 0.0274\n",
      "Epoch: 005/020 | Train: 99.150%\n",
      "Time elapsed: 2.12 min\n",
      "Epoch: 006/020 | Batch 0000/0469 | Cost: 0.0252\n",
      "Epoch: 006/020 | Batch 0050/0469 | Cost: 0.0590\n",
      "Epoch: 006/020 | Batch 0100/0469 | Cost: 0.0559\n",
      "Epoch: 006/020 | Batch 0150/0469 | Cost: 0.0338\n",
      "Epoch: 006/020 | Batch 0200/0469 | Cost: 0.0578\n",
      "Epoch: 006/020 | Batch 0250/0469 | Cost: 0.0239\n",
      "Epoch: 006/020 | Batch 0300/0469 | Cost: 0.0351\n",
      "Epoch: 006/020 | Batch 0350/0469 | Cost: 0.0543\n",
      "Epoch: 006/020 | Batch 0400/0469 | Cost: 0.1036\n",
      "Epoch: 006/020 | Batch 0450/0469 | Cost: 0.1057\n",
      "Epoch: 006/020 | Train: 99.137%\n",
      "Time elapsed: 2.53 min\n",
      "Epoch: 007/020 | Batch 0000/0469 | Cost: 0.0233\n",
      "Epoch: 007/020 | Batch 0050/0469 | Cost: 0.0192\n",
      "Epoch: 007/020 | Batch 0100/0469 | Cost: 0.0318\n",
      "Epoch: 007/020 | Batch 0150/0469 | Cost: 0.0255\n",
      "Epoch: 007/020 | Batch 0200/0469 | Cost: 0.0624\n",
      "Epoch: 007/020 | Batch 0250/0469 | Cost: 0.0708\n",
      "Epoch: 007/020 | Batch 0300/0469 | Cost: 0.0156\n",
      "Epoch: 007/020 | Batch 0350/0469 | Cost: 0.0213\n",
      "Epoch: 007/020 | Batch 0400/0469 | Cost: 0.0275\n",
      "Epoch: 007/020 | Batch 0450/0469 | Cost: 0.0409\n",
      "Epoch: 007/020 | Train: 99.200%\n",
      "Time elapsed: 2.93 min\n",
      "Epoch: 008/020 | Batch 0000/0469 | Cost: 0.0165\n",
      "Epoch: 008/020 | Batch 0050/0469 | Cost: 0.0046\n",
      "Epoch: 008/020 | Batch 0100/0469 | Cost: 0.0469\n",
      "Epoch: 008/020 | Batch 0150/0469 | Cost: 0.0231\n",
      "Epoch: 008/020 | Batch 0200/0469 | Cost: 0.0120\n",
      "Epoch: 008/020 | Batch 0250/0469 | Cost: 0.0081\n",
      "Epoch: 008/020 | Batch 0300/0469 | Cost: 0.1614\n",
      "Epoch: 008/020 | Batch 0350/0469 | Cost: 0.0675\n",
      "Epoch: 008/020 | Batch 0400/0469 | Cost: 0.0283\n",
      "Epoch: 008/020 | Batch 0450/0469 | Cost: 0.1157\n",
      "Epoch: 008/020 | Train: 99.372%\n",
      "Time elapsed: 3.32 min\n",
      "Epoch: 009/020 | Batch 0000/0469 | Cost: 0.0183\n",
      "Epoch: 009/020 | Batch 0050/0469 | Cost: 0.0051\n",
      "Epoch: 009/020 | Batch 0100/0469 | Cost: 0.0023\n",
      "Epoch: 009/020 | Batch 0150/0469 | Cost: 0.0569\n",
      "Epoch: 009/020 | Batch 0200/0469 | Cost: 0.0164\n",
      "Epoch: 009/020 | Batch 0250/0469 | Cost: 0.0758\n",
      "Epoch: 009/020 | Batch 0300/0469 | Cost: 0.0169\n",
      "Epoch: 009/020 | Batch 0350/0469 | Cost: 0.0219\n",
      "Epoch: 009/020 | Batch 0400/0469 | Cost: 0.0495\n",
      "Epoch: 009/020 | Batch 0450/0469 | Cost: 0.0662\n",
      "Epoch: 009/020 | Train: 99.570%\n",
      "Time elapsed: 3.71 min\n",
      "Epoch: 010/020 | Batch 0000/0469 | Cost: 0.0151\n",
      "Epoch: 010/020 | Batch 0050/0469 | Cost: 0.0137\n",
      "Epoch: 010/020 | Batch 0100/0469 | Cost: 0.0175\n",
      "Epoch: 010/020 | Batch 0150/0469 | Cost: 0.0311\n",
      "Epoch: 010/020 | Batch 0200/0469 | Cost: 0.0428\n",
      "Epoch: 010/020 | Batch 0250/0469 | Cost: 0.0240\n",
      "Epoch: 010/020 | Batch 0300/0469 | Cost: 0.0750\n",
      "Epoch: 010/020 | Batch 0350/0469 | Cost: 0.0516\n",
      "Epoch: 010/020 | Batch 0400/0469 | Cost: 0.0038\n",
      "Epoch: 010/020 | Batch 0450/0469 | Cost: 0.0370\n",
      "Epoch: 010/020 | Train: 99.325%\n",
      "Time elapsed: 4.12 min\n",
      "Epoch: 011/020 | Batch 0000/0469 | Cost: 0.0046\n",
      "Epoch: 011/020 | Batch 0050/0469 | Cost: 0.0042\n",
      "Epoch: 011/020 | Batch 0100/0469 | Cost: 0.0078\n",
      "Epoch: 011/020 | Batch 0150/0469 | Cost: 0.0560\n",
      "Epoch: 011/020 | Batch 0200/0469 | Cost: 0.0052\n",
      "Epoch: 011/020 | Batch 0250/0469 | Cost: 0.0683\n",
      "Epoch: 011/020 | Batch 0300/0469 | Cost: 0.0097\n",
      "Epoch: 011/020 | Batch 0350/0469 | Cost: 0.0212\n",
      "Epoch: 011/020 | Batch 0400/0469 | Cost: 0.0207\n",
      "Epoch: 011/020 | Batch 0450/0469 | Cost: 0.0763\n",
      "Epoch: 011/020 | Train: 99.382%\n",
      "Time elapsed: 4.52 min\n",
      "Epoch: 012/020 | Batch 0000/0469 | Cost: 0.0083\n",
      "Epoch: 012/020 | Batch 0050/0469 | Cost: 0.0013\n",
      "Epoch: 012/020 | Batch 0100/0469 | Cost: 0.0145\n",
      "Epoch: 012/020 | Batch 0150/0469 | Cost: 0.0169\n",
      "Epoch: 012/020 | Batch 0200/0469 | Cost: 0.0225\n",
      "Epoch: 012/020 | Batch 0250/0469 | Cost: 0.0272\n",
      "Epoch: 012/020 | Batch 0300/0469 | Cost: 0.0022\n",
      "Epoch: 012/020 | Batch 0350/0469 | Cost: 0.0685\n",
      "Epoch: 012/020 | Batch 0400/0469 | Cost: 0.0608\n",
      "Epoch: 012/020 | Batch 0450/0469 | Cost: 0.0379\n",
      "Epoch: 012/020 | Train: 99.500%\n",
      "Time elapsed: 4.93 min\n",
      "Epoch: 013/020 | Batch 0000/0469 | Cost: 0.0367\n",
      "Epoch: 013/020 | Batch 0050/0469 | Cost: 0.0071\n",
      "Epoch: 013/020 | Batch 0100/0469 | Cost: 0.0072\n",
      "Epoch: 013/020 | Batch 0150/0469 | Cost: 0.0008\n",
      "Epoch: 013/020 | Batch 0200/0469 | Cost: 0.0874\n",
      "Epoch: 013/020 | Batch 0250/0469 | Cost: 0.0534\n",
      "Epoch: 013/020 | Batch 0300/0469 | Cost: 0.0404\n",
      "Epoch: 013/020 | Batch 0350/0469 | Cost: 0.0027\n",
      "Epoch: 013/020 | Batch 0400/0469 | Cost: 0.0059\n",
      "Epoch: 013/020 | Batch 0450/0469 | Cost: 0.0061\n",
      "Epoch: 013/020 | Train: 99.807%\n",
      "Time elapsed: 5.34 min\n",
      "Epoch: 014/020 | Batch 0000/0469 | Cost: 0.0227\n",
      "Epoch: 014/020 | Batch 0050/0469 | Cost: 0.0206\n",
      "Epoch: 014/020 | Batch 0100/0469 | Cost: 0.0063\n",
      "Epoch: 014/020 | Batch 0150/0469 | Cost: 0.0228\n",
      "Epoch: 014/020 | Batch 0200/0469 | Cost: 0.0145\n",
      "Epoch: 014/020 | Batch 0250/0469 | Cost: 0.0013\n",
      "Epoch: 014/020 | Batch 0300/0469 | Cost: 0.0453\n",
      "Epoch: 014/020 | Batch 0350/0469 | Cost: 0.0024\n",
      "Epoch: 014/020 | Batch 0400/0469 | Cost: 0.0020\n",
      "Epoch: 014/020 | Batch 0450/0469 | Cost: 0.0337\n",
      "Epoch: 014/020 | Train: 99.505%\n",
      "Time elapsed: 5.73 min\n",
      "Epoch: 015/020 | Batch 0000/0469 | Cost: 0.0338\n",
      "Epoch: 015/020 | Batch 0050/0469 | Cost: 0.0221\n",
      "Epoch: 015/020 | Batch 0100/0469 | Cost: 0.0089\n",
      "Epoch: 015/020 | Batch 0150/0469 | Cost: 0.0035\n",
      "Epoch: 015/020 | Batch 0200/0469 | Cost: 0.0761\n",
      "Epoch: 015/020 | Batch 0250/0469 | Cost: 0.0187\n",
      "Epoch: 015/020 | Batch 0300/0469 | Cost: 0.0184\n",
      "Epoch: 015/020 | Batch 0350/0469 | Cost: 0.0184\n",
      "Epoch: 015/020 | Batch 0400/0469 | Cost: 0.0065\n",
      "Epoch: 015/020 | Batch 0450/0469 | Cost: 0.0065\n",
      "Epoch: 015/020 | Train: 99.588%\n",
      "Time elapsed: 6.12 min\n",
      "Epoch: 016/020 | Batch 0000/0469 | Cost: 0.0660\n",
      "Epoch: 016/020 | Batch 0050/0469 | Cost: 0.0166\n",
      "Epoch: 016/020 | Batch 0100/0469 | Cost: 0.0015\n",
      "Epoch: 016/020 | Batch 0150/0469 | Cost: 0.0194\n",
      "Epoch: 016/020 | Batch 0200/0469 | Cost: 0.1045\n",
      "Epoch: 016/020 | Batch 0250/0469 | Cost: 0.0023\n",
      "Epoch: 016/020 | Batch 0300/0469 | Cost: 0.0156\n",
      "Epoch: 016/020 | Batch 0350/0469 | Cost: 0.0550\n",
      "Epoch: 016/020 | Batch 0400/0469 | Cost: 0.0097\n",
      "Epoch: 016/020 | Batch 0450/0469 | Cost: 0.0125\n",
      "Epoch: 016/020 | Train: 99.665%\n",
      "Time elapsed: 6.51 min\n",
      "Epoch: 017/020 | Batch 0000/0469 | Cost: 0.0223\n",
      "Epoch: 017/020 | Batch 0050/0469 | Cost: 0.0009\n",
      "Epoch: 017/020 | Batch 0100/0469 | Cost: 0.0043\n",
      "Epoch: 017/020 | Batch 0150/0469 | Cost: 0.0047\n",
      "Epoch: 017/020 | Batch 0200/0469 | Cost: 0.0075\n",
      "Epoch: 017/020 | Batch 0250/0469 | Cost: 0.0063\n",
      "Epoch: 017/020 | Batch 0300/0469 | Cost: 0.0050\n",
      "Epoch: 017/020 | Batch 0350/0469 | Cost: 0.0177\n",
      "Epoch: 017/020 | Batch 0400/0469 | Cost: 0.0002\n",
      "Epoch: 017/020 | Batch 0450/0469 | Cost: 0.0052\n",
      "Epoch: 017/020 | Train: 99.293%\n",
      "Time elapsed: 6.91 min\n",
      "Epoch: 018/020 | Batch 0000/0469 | Cost: 0.0024\n",
      "Epoch: 018/020 | Batch 0050/0469 | Cost: 0.0288\n",
      "Epoch: 018/020 | Batch 0100/0469 | Cost: 0.0007\n",
      "Epoch: 018/020 | Batch 0150/0469 | Cost: 0.0003\n",
      "Epoch: 018/020 | Batch 0200/0469 | Cost: 0.0143\n",
      "Epoch: 018/020 | Batch 0250/0469 | Cost: 0.0007\n",
      "Epoch: 018/020 | Batch 0300/0469 | Cost: 0.0198\n",
      "Epoch: 018/020 | Batch 0350/0469 | Cost: 0.0085\n",
      "Epoch: 018/020 | Batch 0400/0469 | Cost: 0.0101\n",
      "Epoch: 018/020 | Batch 0450/0469 | Cost: 0.0016\n",
      "Epoch: 018/020 | Train: 99.727%\n",
      "Time elapsed: 7.30 min\n",
      "Epoch: 019/020 | Batch 0000/0469 | Cost: 0.0357\n",
      "Epoch: 019/020 | Batch 0050/0469 | Cost: 0.0306\n",
      "Epoch: 019/020 | Batch 0100/0469 | Cost: 0.0035\n",
      "Epoch: 019/020 | Batch 0150/0469 | Cost: 0.0137\n",
      "Epoch: 019/020 | Batch 0200/0469 | Cost: 0.0349\n",
      "Epoch: 019/020 | Batch 0250/0469 | Cost: 0.0450\n",
      "Epoch: 019/020 | Batch 0300/0469 | Cost: 0.0094\n",
      "Epoch: 019/020 | Batch 0350/0469 | Cost: 0.0184\n",
      "Epoch: 019/020 | Batch 0400/0469 | Cost: 0.0079\n",
      "Epoch: 019/020 | Batch 0450/0469 | Cost: 0.0014\n",
      "Epoch: 019/020 | Train: 99.740%\n",
      "Time elapsed: 7.69 min\n",
      "Epoch: 020/020 | Batch 0000/0469 | Cost: 0.0007\n",
      "Epoch: 020/020 | Batch 0050/0469 | Cost: 0.0020\n",
      "Epoch: 020/020 | Batch 0100/0469 | Cost: 0.0046\n",
      "Epoch: 020/020 | Batch 0150/0469 | Cost: 0.0060\n",
      "Epoch: 020/020 | Batch 0200/0469 | Cost: 0.0017\n",
      "Epoch: 020/020 | Batch 0250/0469 | Cost: 0.0117\n",
      "Epoch: 020/020 | Batch 0300/0469 | Cost: 0.0038\n",
      "Epoch: 020/020 | Batch 0350/0469 | Cost: 0.0042\n",
      "Epoch: 020/020 | Batch 0400/0469 | Cost: 0.0093\n",
      "Epoch: 020/020 | Batch 0450/0469 | Cost: 0.0281\n",
      "Epoch: 020/020 | Train: 99.820%\n",
      "Time elapsed: 8.09 min\n",
      "Total Training Time: 8.09 min\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "    model.train()\n",
    "    for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "        \n",
    "        features = features.to(DEVICE)\n",
    "        targets = targets.to(DEVICE)\n",
    "            \n",
    "        ### FORWARD AND BACK PROP\n",
    "        logits, probas = model(features)\n",
    "        cost = F.cross_entropy(logits, targets)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        cost.backward()\n",
    "        \n",
    "        ### UPDATE MODEL PARAMETERS\n",
    "        optimizer.step()\n",
    "        \n",
    "        ### LOGGING\n",
    "        if not batch_idx % 50:\n",
    "            print ('Epoch: %03d/%03d | Batch %04d/%04d | Cost: %.4f' \n",
    "                   %(epoch+1, NUM_EPOCHS, batch_idx, \n",
    "                     len(train_loader), cost))\n",
    "\n",
    "    model.eval()\n",
    "    with torch.set_grad_enabled(False): # save memory during inference\n",
    "        print('Epoch: %03d/%03d | Train: %.3f%%' % (\n",
    "              epoch+1, NUM_EPOCHS, \n",
    "              compute_accuracy(model, train_loader, device=DEVICE)))\n",
    "        \n",
    "    print('Time elapsed: %.2f min' % ((time.time() - start_time)/60))\n",
    "    \n",
    "print('Total Training Time: %.2f min' % ((time.time() - start_time)/60))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guardar el modelo entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'weights/mnist.pth')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluar el modelo entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([16])\n",
      "correctos:  0 | total:  0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mFalse\u001b[39;00m): \u001b[39m# save memory during inference\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mTest accuracy: \u001b[39m\u001b[39m%.2f\u001b[39;00m\u001b[39m%%\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m (compute_accuracy(model, test_loader, device\u001b[39m=\u001b[39;49mDEVICE)))\n",
      "Cell \u001b[0;32mIn[53], line 14\u001b[0m, in \u001b[0;36mcompute_accuracy\u001b[0;34m(model, data_loader, device)\u001b[0m\n\u001b[1;32m     12\u001b[0m         correct_pred \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (predicted_labels \u001b[39m==\u001b[39m targets)\u001b[39m.\u001b[39msum()\n\u001b[1;32m     13\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mcorrectos: \u001b[39m\u001b[39m\"\u001b[39m, correct_pred ,\u001b[39m'\u001b[39m\u001b[39m| total: \u001b[39m\u001b[39m'\u001b[39m,num_examples )\n\u001b[0;32m---> 14\u001b[0m \u001b[39mreturn\u001b[39;00m correct_pred\u001b[39m.\u001b[39;49mfloat() \u001b[39m/\u001b[39m num_examples \u001b[39m*\u001b[39m \u001b[39m100\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'float'"
     ]
    }
   ],
   "source": [
    "with torch.set_grad_enabled(False): # save memory during inference\n",
    "    print('Test accuracy: %.2f%%' % (compute_accuracy(model, test_loader, device=DEVICE)))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar pesos y evaluar dnvo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 98.46%\n"
     ]
    }
   ],
   "source": [
    "# Crear una instancia del modelo con la misma arquitectura\n",
    "model = resnet50(NUM_CLASSES)\n",
    "model.to(DEVICE)\n",
    "\n",
    "# Cargar los pesos del modelo\n",
    "model.load_state_dict(torch.load('weights/mnist.pth'))\n",
    "\n",
    "# Cambiar el modelo a eval() para usarlo en inferencia\n",
    "with torch.set_grad_enabled(False): # save memory during inference\n",
    "    print('Test accuracy: %.2f%%' % (compute_accuracy(model, test_loader, device=DEVICE)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
